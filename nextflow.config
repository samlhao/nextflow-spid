/*
 * -------------------------------------------------
 *  nf-core/spid Nextflow config file
 * -------------------------------------------------
 * Default config options for all environments.
 */

// Global default params, used in configs
params {

  // Workflow flags
  // TODO nf-core: Specify your pipeline's command line flags
  genome = false
  se_reads = false
  pe_reads = false
  assemblies = false
  outdir = './results'
  mlst_db = false
  mlst_def = false
  amr_db = false
  fasta = false
  skip_trimming = false
  forward_suffix = '_R1'
  reverse_suffix = '_R2'
  readsdir = outdir


  // Boilerplate options
  name = false
  multiqc_config = "$baseDir/assets/multiqc_config.yaml"
  email = false
  email_on_fail = false
  maxMultiqcEmailFileSize = 25.MB
  plaintext_email = false
  monochrome_logs = false
  help = false
  igenomes_base = "./iGenomes"
  tracedir = "${params.outdir}/pipeline_info"
  awsqueue = false
  awsregion = 'us-west-2'
  igenomesIgnore = false
  custom_config_version = 'master'
  custom_config_base = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
  hostnames = false
  config_profile_description = false
  config_profile_contact = false
  config_profile_url = false
}

// Container slug. Stable releases should specify release tag!
// Developmental code should specify :dev
// process.container = 'nfcore/spid:dev'
process {
  container = 'czbiohub/nf-spid:dev'
  withLabel: srst2 {
    container = 'quay.io/biocontainers/srst2:0.2.0--py27_2'
    // container = 'samlhao/nf-spid:dev'    
   // conda = "$baseDir/srst2_env.yml"
  }
  withLabel: 'spid_docker' {
    container = 'czbiohub/spid:latest'
  }
}

// latest base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load nf-core custom profiles from different Institutions
try {
  includeConfig "${params.custom_config_base}/nfcore_custom.config"
} catch (Exception e) {
  System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
}

profiles {
  awsbatch { includeConfig 'conf/aws.config' }
  conda { process.conda = "$baseDir/environment.yml" }
  debug { process.beforeScript = 'echo $HOSTNAME' }
  docker { docker.enabled = true }
  singularity { singularity.enabled = true }
  test { includeConfig 'conf/test.config' }
  test_id_species { includeConfig 'conf/test_id_species.config' }
}

// Avoid this error:
// WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.
// Testing this in nf-core after discussion here https://github.com/nf-core/tools/pull/351, once this is established and works well, nextflow might implement this behavior as new default.
docker.runOptions = '-u \$(id -u):\$(id -g)'

// Load igenomes.config if required
if (!params.igenomesIgnore) {
  includeConfig 'conf/igenomes.config'
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

timeline {
  enabled = true
  file = "${params.tracedir}/execution_timeline.html"
}
report {
  enabled = true
  file = "${params.tracedir}/execution_report.html"
}
trace {
  enabled = true
  file = "${params.tracedir}/execution_trace.txt"
}
dag {
  enabled = true
  file = "${params.tracedir}/pipeline_dag.svg"
}

manifest {
  name = 'czbiohub/nextflow-spid'
  author = 'Samantha Hao'
  homePage = 'https://github.com/czbiohub/nextflow-spid'
  description = 'SPID nextflow pipeline'
  mainScript = 'main.nf'
  nextflowVersion = '>=0.32.0'
  version = '1.0dev'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}

process {

  cpus = { check_max( 1 * task.attempt, 'cpus') }
  memory = { check_max( 8.GB * task.attempt, 'memory') }
  time = { check_max( 2.h * task.attempt, 'time') }


  maxRetries = 3
  maxErrors = '-1'

  // Resource requirements
  withName: fastqc {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }
  withLabel: fastp {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }
  withLabel: srst2 {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 8
  }
  withLabel: srst2 {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 8
  }
  withName: compile_srst2 {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }  
  withName: bgzip_fasta {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }
  withName: samtools_faidx {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }
  withName: generate_consensus_sr {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }
  withName: merge_sample_fastas {
    memory = { check_max( 64.GB * task.attempt, 'memory') }
    cpus = 32
  }
  withName: count_num_sites {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 1
  }
  withName: raxml {
    memory = { check_max( 8.GB * task.attempt, 'memory') }
    cpus = 32
  }  

}
